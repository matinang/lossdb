---
output: 
  html_document:
    theme: flatly
---
<!--
%\VignetteIndexEntry{Introduction to lossdb}
%\VignetteEngine{knitr::rmarkdown}
-->

```{r, echo = FALSE, message = FALSE}
library(lossdb)
library(knitr)
knitr::opts_chunk$set(
  comment = "#>",
  tidy = FALSE)
```

# Introduction to lossdb

The goal of this document is to introduce the basic tools `lossdb` provides, and to walk through a realistic example using the `lossdb` package.  This vignette and package are still in their early stages of development...

## Background: Actuaries and Insurance Loss Data

The largest and most important data sets actuaries commonly work with are data sets describing the value and nature of insurance losses.  These data sets are often provided on a "by claim" basis (i.e. each row represents a claim and each column a variable).  These "by claim" data data sets, which I will call "loss data", are provided in a different format by every company, and often times many different formats by the same company.  A thorough combing of the loss data is often necessary to determine what each column is communicating and what columns are relevant to the analysis.  Additionally there are almost always errors that need to be addressed.  Understanding the underlying structure of the columns and ways to identify common errors can significantly improve the overall analysis process.

## Motivation

I want to make reproducible actuarial reports in R.  Getting unclean data ready for analysis is a pain, and once the data is clean it is nice to have it in some kind of a standard format so you can do similar things to data whenever you get it into that format... enter `lossdb`

The goal of the `lossdb` package is to provide standard functions for manipulating and visualizing loss data. `lossdb` provides a standard means of storing your claims data so you can use the same set of functions (defined by `lossdb`) to automate many repetetive tasks and ultimately create reproducible reports that are generated directly from the original loss data.

When using R one can easily improve the flexibility of their reports by maintaining access to all the original claim detail (i.e. each report can be generated from the original claim data as opposed to a summary and a bunch of copy and pastes of the original claim data). This can be a huge time saver if you need to go back and change an origninal assumtion of your report.  Additionally generating reports from the original data reduces errors and provides confidence that the numbers in your report are up to date.  

In summary, `lossdb` provides a standard format for storing loss data and provides a set of functions specific to that standard format.  The fact that the entire report can be done in R is what makes it more easily reproducible.  `lossdb` makes it a little easier to go from raw data to a final report in R.

## Philosophy

Columns in loss data can be losely grouped into a few different categories.  By organizing each column into one of these categories (defined below) many common actuarial analysis can be automated.  These actuarial analysis can then be scaled to any data set with columns defined by the same structure. 

### The `lossdb` structure

`lossdb` categorizes loss data columns into one of 3 main categories:

* dollar
* desc
* meta

#### dollar

I refer to all columns representing dollar amounts as the "dollar" columns.  `lossdb` assumes that all the information contained in the dollar columns can be sub categorized into 4 different groups without losing any information.  Each of these 4 groups can contain as many columns as neccessary.  The 4 groups:

* paid - actual amounts already paid
* incurred - amounts incurred (i.e. total paid + reserve expected to be paid by the insurance company before the claim is closed)
* paid_recovery - actual paid reimbursements (e.g. amount under deductible)
* incurred_recovery - incurred reimbursements

#### desc

Other columns included in loss data provide some type of description of the claim (e.g. claimant name, whether the claim is open or closed, etc.).  I refer to all of these columns as "desc" (short for description) columns.  The `lossdb` package can handle any number of description columns.

#### meta

Additionally there are 3 special columns of particular importance to actuarial analysis.  I refer to these columns as the "meta" columns and they are defined as follows:

* origin - the accident period or policy period in which the claim/occurence originated
* dev - the development stage of the claim
* id - a unique (by dev) identifier for the claim

In summary `lossdb` organizes all the variables descibing loss data into one of these categories:

* meta
    + id - optional - factor - single column
    + dev - required - numeric - single column
    + origin - required - numeric - single column
* detail
    + dollar
        + paid - optional - numeric - supports multiple columns
        + incurred - optional - numeric - supports multiple columns
        + paid_recovery - optional - numeric - supports multiple columns
        + incurred_recovery - optional - numeric - supports multiple columns
    + desc - optional - numeric - supports multiple columns

As you can see, `dollar` and `desc` fall under `detail` in the above tree.  `detail` is used to represent both the `dollar` and `desc` categories, but you do not need to know this to use the package.

## Getting Started

The standard format `lossdb` uses to store claim loss data is the S3 class `loss_df`.  A `loss_df` is created using the `loss_df()` function.  A `loss_df` holds and cetegorizes loss information on a claim or occurence basis, and is flexible enough to handle data that is categorized and organized differently from company to company. 

The rest of this vignette will guide you through an example using `lossdb`.  The example will proceed as follows:

* Take a data set (`losses`) in a format resembling how data may be provided by an insurance company, and transforming that data set into a `loss_df` object that contains and organizes all the information relevant to the analysis.
* Review the `loss_df` for errors and potential problem areas.
* Perform statistical reserving techniques on the `loss_df`.

View the structure of the uncleaned `losses` data frame using the `str` function:
```{r, eval = FALSE}
str(losses)
```

There are many columns that are not relevant to our analysis, and some transformations must be applied to the data to get all the necessary information out of the losses data frame.  Let's get started.

### Format the data 

The `loss_df()` function takes the data frame columns relevant to the actuarial analysis as as its formal arguments.  Before we can create a `loss_df` object we need to transofrm the data set into a usable format.

We use the dplyr package to clean the `losses` data so it conforms to the format required to make a `loss_df`.  If you are not familiar with the dplyr package see the [dplyr introduction vignette](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)

```{r, message = FALSE}
library(dplyr)
# create origin and dev column
losses <- mutate(losses, origin = as.numeric(substr(fiscal_year_desc, 1, 4)), 
                 evaluation_year = as.numeric(format(as.Date(evaluation_date, "%Y-%m-%d"), "%Y")),
                 dev = evaluation_year - origin) 

# group claims by occurence
# often necessary when excess reinsurance is applied on an occurence basis
# rather than on a claims basis.
occurences <- losses %>%
  group_by(claim_number, dev, origin) %>%
     summarise(claim_cts = n(),
               payment_amount = sum(payment_amount), # paid loss & ALAE
               incurred_amount = sum(reserve_amount), # incurred loss & ALAE
               paid_expense = sum(X4_exp_payment), # paid ALAE
               incurred_expense = sum(X4_exp_reserve), #incurred ALAE
               sal_sub_paid = sum(payment_no_reserve_a), # salvage and subrogation
               sal_sub_incurred = sum(payment_no_reserve_a)
              )

# create relevant loss values columns
occurences <- mutate(occurences,
                    paid_loss = payment_amount - paid_expense,
                    incurred_loss = incurred_amount - incurred_expense,
                    paid_excess250 = max(payment_amount - sal_sub_paid - 250000, 0),
                    incurred_excess250 = max(incurred_amount - sal_sub_incurred - 250000, 0))

# need to get rid of grouped df class. causing problems with subsetting.
occurences <- as.data.frame(occurences)
```

The above code does not use the `lossdb` package.  All that code did was prepare the data for use with the `lossdb` package (often a time consuming task esprecially the first time working with a new data set).

Now that we have all the necessary columns, we can create the `loss_df` object.
```{r}
# create loss_df object
mydf <- loss_df(occurences, id = "claim_number",
                             origin = "origin",
                             dev = "dev", 
                             paid = c("paid_loss", "paid_expense"),
                             incurred = c("incurred_loss", "incurred_expense"),
                             paid_recovery = c("paid_excess250", "sal_sub_paid"),
                             incurred_recovery = c("incurred_excess250", 
                                                   "sal_sub_incurred"),
                             desc = "claim_cts"
                 )
head(mydf[, 1:6])
```

Each detail (dollar or desc) column has an attribute, `detail`, specifying the type of loss detail that the column contains.  The `detail` attribute is defined by the argument the column is supplied to in the `loss_df()` function (i.e. `paid_loss` and `paid_expense` are of `detail` `paid`).  These column maintain the column names that they were supplied with, but are grouped by there `detail` category.  The names for `meta` columns are changes to the `meta` category they were supplied to.

### Review the data

Now we can use the `lossdb` package to review the data.  Let's start by seeing a summary of the most recent `calendar` period (`calendar` = `origin` + `dev`) summarized by `origin` period.

```{r, results = 'asis'}
kable(summary(mydf))
```

We can look at the data at an older `calendar` period by specifying the `calendar` argument in the `summary()` function.

```{r, results = 'asis'}
kable(summary(mydf, calendar = "2012"))
```

Note: the `calendar` period is the `origin` period plus the `dev`. (e.g.) The `calendar` for all claims in origin year 2010 at their first `calendar` period would be 2011.

and the built in bar chart representation of the data...

```{r}
plot(mydf)
```

and plotted at an alternative `calendar`

```{r}
plot(mydf, calendar = "2012")
```

We can return a data frame of all the claims that have experienced a change from one evaluation date to another by using the `claim_changes()` function:

```{r}
# specify the loss amount values you want to see the changed claims for 
mychanges <- claim_changes(mydf, calendar1 = "2013", calendar2 = "2012",
                            values = c("paid_loss", "claim_cts"))
head(mychanges)
```

`mychanges` is a data frame consisting of all the claims in which there was a change in the `paid_loss` column from calendar period 2013 to 2014.  You can now browse through the changed claims to spot obvious problems with the new data.  For example we may want to check that there are no missing claims (i.e. no claims that were in the data at the last `calendar` that are no longer in the data)

```{r}
# check for missing claims
mychanges[, mychanges$claim_cts_change < 0]
```

We may also want to check if the `paid_loss` category decreased for any claims.

```{r}
# check for claims in which paid_loss decreased
mychanges[, mychanges$paid_loss_changes < 0]
```

There are no missing claims or claims with a decrease in `paid_loss`.  After this quick review it appears that new data are reasonable.  Next we can project some ultimate losses. 

### Create Projections

Before a projection is made we must specifiy the loss amounts we wish to project (e.g. paid loss & ALAE gross of all recoveries, paid loss & ALAE net of all recoveries, medical only paid loss & ALAE gross of all recoveries, etc.).  Use the `paid()`, `incurred()`, `paid_recovery()`, and `incurred_recovery()` functions to get the total from each category.

```{r}
# project total paid losses gross of any recovery
value2project <- data.frame(origin = mydf$origin, dev = mydf$dev, paid_total = paid(mydf))
head(value2project)
```

Now the `ChainLadder` package can be used to make projections.  

```{r}
suppressMessages(library(ChainLadder))
paid_tri <- as.triangle(value2project, origin = "origin", dev = "dev", value = "paid_total")
```

```{r}
MackChainLadder(paid_tri)
```